# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13p4J2AUg7Z4avaoUvxdc_8S3IEVLi_a8
"""

from spacy.language import Language
import spacy
from bs4 import BeautifulSoup

!wget 'https://en.wikipedia.org/wiki/Natural_language_processing' -O site1.html
!wget 'https://www.datarobot.com/blog/what-is-natural-language-processing-introduction-to-nlp/' -O site2.html
!wget 'https://hbr.org/2022/04/the-power-of-natural-language-processing' -O site3.html
!wget 'https://www.wonderflow.ai/blog/natural-language-processing-examples' -O site4.html
!wget 'https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP' -O site5.html

def criacaoFuncao(numerosite):
  while numerosite < 5:
    numerosite = numerosite + 1
    text = "".join([a.get_text() for a in BeautifulSoup(open("./site{}.html".format(numerosite), 'r'.format(numerosite)), 'html.parser').find_all('p')])
    nlp = spacy.load("en_core_web_sm")
    nlp.add_pipe("set", before="parser")
    doc = nlp(text)
    array2 = [sent.text for sent in nlp(text).sents]
    array2 = len(array2)
    print("array de sentenças :", [sent.text for sent in doc.sents])
    print(len(text), " palavras - ", array2, " sentenças no array de sentenças \n")

@Language.component("set")
def set(doc):
    for token in doc[:-1]:
        if token.text == "," or token.text == "..." or token.text == "?" or token.text == "!":
            doc[token.i + 1].is_sent_start = True
    return doc
criacaoFuncao(0)

import numpy
import re

def tokenize(sentences):
    words = []
    for sentence in sentences:
        w = word_extraction(sentence)
        words.extend(w)
    l = []
    for i in words:
        if i not in l:
            l.append(i)
    l.sort()
    return l

def word_extraction(sentence):
    ignore = ['a', "the", "is"]
    words = re.sub("[^\w]", " ",  sentence).split()
    limpar = [w.lower() for w in words if w not in ignore]
    return limpar    
    
resultado = []
allsentences = []
contador = 0 
while contador < 5:
  contador = contador + 1
  text = "".join([a.get_text() for a in BeautifulSoup(open("./site{}.html".format(contador), 'r'.format(contador)), 'html.parser').find_all('p')])
  nlp = spacy.load("en_core_web_sm")
  nlp.add_pipe("set", before="parser")
  doc = nlp(text)
  allsentences.append([sent.text for sent in nlp(text).sents])
    
allsentences = [sent.text for sent in nlp(text).sents]
vocab = tokenize(allsentences)
for sentence in allsentences:
    words = word_extraction(sentence)
    bag_vector = numpy.zeros(len(vocab))
    for w in words:
        for i,word in enumerate(vocab):
            if word == w: 
                bag_vector[i] += 1
    resultado.append(numpy.array(bag_vector))
import spacy
nlp=spacy.load("en_core_web_sm")
import pandas as pd
df = pd.DataFrame(resultado, columns=vocab)
df